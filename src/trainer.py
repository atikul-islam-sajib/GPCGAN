import sys
import logging
import argparse
import numpy as np
import torch
import joblib
import torch.nn as nn
import torch.optim as optim

sys.path.append("src/")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    filemode="w",
    filename="./logs/trainer.log",
)

from generator import Generator
from discriminator import Discriminator


class Trainer:
    """
    The `Trainer` class encapsulates the training process for a conditional Generative Adversarial Network (cGAN) composed of a generator and a discriminator. It manages the training loop, loss computations, and parameter updates.

    ## Attributes:
    - `latent_space` (int): Dimensionality of the generator's input latent space.
    - `epochs` (int): Total number of training epochs.
    - `learning_rate` (float): Learning rate for the Adam optimizers.
    - `beta1` (float): Beta1 hyperparameter for the Adam optimizer.
    - `beta2` (float): Beta2 hyperparameter for the Adam optimizer.
    - `generator` (Generator): Generator model of the GAN.
    - `discriminator` (Discriminator): Discriminator model of the GAN.
    - `dataloader` (DataLoader): DataLoader providing the training data and labels.
    - `loss_function` (nn.Module): Binary Cross Entropy loss function for training the discriminator and generator.
    - `optimizer_generator` (optim.Optimizer): Optimizer for updating the generator's weights.
    - `optimizer_discriminator` (optim.Optimizer): Optimizer for updating the discriminator's weights.
    - `discriminator_loss` (list): List to record the discriminator's loss after each epoch.
    - `generator_loss` (list): List to record the generator's loss after each epoch.

    ## Methods:
    Detailed descriptions of each method are provided below.
    """

    def __init__(self, latent_space=100, epochs=100, lr=0.0002, beta1=0.5, beta2=0.999):
        """
        Initializes the Trainer object with the specified configuration and sets up the neural network models, dataloader, loss function, and optimizers.

        ### Parameters:
        - `latent_space` (int): Size of the latent space (input vector for the generator).
        - `epochs` (int): Number of epochs for training the models.
        - `lr` (float): Learning rate for the Adam optimizers.
        - `beta1` (float): Beta1 hyperparameter for the Adam optimizer.
        - `beta2` (float): Beta2 hyperparameter for the Adam optimizer.
        """
        self.latent_space = latent_space
        self.epochs = epochs
        self.learning_rate = lr
        self.beta1 = beta1
        self.beta2 = beta2

        self.generator = Generator()
        self.discriminator = Discriminator()

        try:
            self.dataloader = joblib.load(filename="./data/processed/dataloader.pkl")
        except Exception as e:
            logging.exception("dataloader is not transformed from pickle".capitalize())

        self.loss_function = nn.BCELoss()
        self.optimizer_generator = optim.Adam(
            params=self.generator.parameters(),
            lr=self.learning_rate,
            betas=(self.beta1, self.beta2),
        )
        self.optimizer_discriminator = optim.Adam(
            params=self.discriminator.parameters(),
            lr=self.learning_rate,
            betas=(self.beta1, self.beta2),
        )

        self.discriminator_loss = list()
        self.generator_loss = list()

    def saved_checkpoints(self, model=None, epoch=None):
        """
        Saves a checkpoint of the given model at the specified epoch.

        ### Parameters:
        - `model` (nn.Module): The model to be saved.
        - `epoch` (int): The current epoch number for naming the saved file.

        ### Side Effects:
        - Saves the model's state dictionary to the file system.
        """
        torch.save(
            model.state_dict(), "./models/checkpoints/generator_{}.pth".format(epoch)
        )

    def train_discriminator(self, **kwargs):
        """
        Trains the discriminator model for one batch of data.

        ### Parameters (passed as keyword arguments):
        - `real_samples` (Tensor): Real samples from the dataset.
        - `labels` (Tensor): Corresponding labels for the real samples.
        - `fake_samples` (Tensor): Fake samples generated by the generator.
        - `real_labels` (Tensor): Tensor of ones, representing real labels.
        - `fake_labels` (Tensor): Tensor of zeros, representing fake labels.

        ### Returns:
        - `total_loss` (Tensor): The total loss for the discriminator for the current batch.

        ### Side Effects:
        - Updates the weights of the discriminator model.
        """
        real_predict = self.discriminator(kwargs["real_samples"], kwargs["labels"])
        fake_predict = self.discriminator(kwargs["fake_samples"], kwargs["labels"])

        real_loss = self.loss_function(real_predict, kwargs["real_labels"])
        fake_loss = self.loss_function(fake_predict, kwargs["fake_labels"])

        total_loss = real_loss + fake_loss

        self.optimizer_discriminator.zero_grad()
        total_loss.backward()
        self.optimizer_discriminator.step()

        return total_loss

    def train_generator(self, **kwargs):
        """
        Trains the generator model for one batch of data.

        ### Parameters (passed as keyword arguments):
        - `generated_samples` (Tensor): Samples generated by the generator.
        - `real_labels` (Tensor): Tensor of ones, representing real labels.
        - `labels` (Tensor): Corresponding labels for the generated samples.

        ### Returns:
        - `generated_loss` (Tensor): The loss for the generator for the current batch.

        ### Side Effects:
        - Updates the weights of the generator model.
        """
        generated_predict = self.discriminator(
            kwargs["generated_samples"], kwargs["labels"]
        )
        generated_loss = self.loss_function(generated_predict, kwargs["real_labels"])

        self.optimizer_generator.zero_grad()
        generated_loss.backward()
        self.optimizer_generator.step()

        return generated_loss

    def train_CGAN(self):
        """
        Conducts the training loop for the Conditional Generative Adversarial Network (cGAN).
        The loop iterates over the dataset, trains the discriminator and generator in alternation,
        and records the loss for each epoch.

        ### Process:
        - For each epoch:
            - For each batch in the dataloader:
                - Train the discriminator using both real and fake data.
                - Generate new fake samples and train the generator.
                - Record and accumulate the loss for both the discriminator and generator.
        - After each epoch, print the average losses and save the generator's state as a checkpoint.

        ### Side Effects:
        - Updates the weights of both the discriminator and generator models.
        - Appends the average loss of each epoch to the respective loss lists (`discriminator_loss`, `generator_loss`).
        - Saves the generator's state after each epoch.
        - Prints the progress and average losses to the console.

        ### Error Handling:
        - If the model checkpoint cannot be saved, an exception is raised.
        """
        for epoch in range(self.epochs):
            d_loss = []
            g_loss = []
            for real_samples, labels in self.dataloader:
                real_samples = real_samples
                labels = labels
                batch_size = real_samples.shape[0]

                real_labels = torch.ones(batch_size, 1)
                fake_labels = torch.zeros(batch_size, 1)

                noise_samples = torch.randn(batch_size, self.latent_space)
                fake_samples = self.generator(noise_samples, labels)

                D_loss = self.train_discriminator(
                    real_labels=real_labels,
                    fake_labels=fake_labels,
                    fake_samples=fake_samples,
                    real_samples=real_samples,
                    labels=labels,
                )

                generated_samples = self.generator(noise_samples, labels)

                G_loss = self.train_generator(
                    generated_samples=generated_samples,
                    real_labels=real_labels,
                    labels=labels,
                )

                d_loss.append(D_loss.item())
                g_loss.append(G_loss.item())

            self.discriminator_loss.append(np.array(d_loss).mean())
            self.generator_loss.append(np.array(g_loss).mean())

            print(f"Epoch [{epoch + 1}/{self.epochs}] Completed")
            print(
                f"[==============] Average d_loss: {(np.array(d_loss).mean()):.4f} - Average g_loss: {(np.array(g_loss).mean()):.4f}"
            )

            try:
                self.saved_checkpoints(model=self.generator, epoch=epoch + 1)
            except Exception as e:
                raise Exception("Model cannot be saved successfully".capitalize())


if __name__ == "__main__":
    """
    # GAN Trainer Script

    This script is responsible for setting up and running the training process of a Generative Adversarial Network (GAN). It includes argument parsing for command-line customization of the training parameters and initiates the training loop for the GAN models.

    ## Features:
    - Command-line argument parsing for flexible training configuration.
    - Conditional checks to ensure valid training parameters.
    - Logging for monitoring the training process.
    - Integration with the `Trainer` class to facilitate the actual training.

    ## Usage:
    To use this script, run it from the command line with the desired arguments, for example:
        python trainer_script.py --epochs 100 --latent_space 100 --lr 0.0002
    ## Arguments:
    - `--epochs`: Number of epochs for training.
    - `--latent_space`: Size of the latent space for the generator.
    - `--lr`: Learning rate for the optimizer.
    """

    parser = argparse.ArgumentParser(description="GAN Training".title())
    parser.add_argument(
        "--epochs", type=int, default=100, help="Number of epochs".capitalize()
    )
    parser.add_argument(
        "--latent_space", type=int, default=100, help="Latent size".capitalize()
    )
    parser.add_argument(
        "--lr", type=float, default=0.0002, help="Learning rate".capitalize()
    )

    args = parser.parse_args()

    if args.epochs > 1 and args.latent_space > 50 and args.lr:
        logging.info("Training started".capitalize())

        trainer = Trainer(
            latent_space=args.latent_space, epochs=args.epochs, lr=args.lr
        )
        trainer.train_CGAN()

        logging.info("Training completed successfully".capitalize())
    else:
        raise Exception("Please check the arguments".capitalize())
